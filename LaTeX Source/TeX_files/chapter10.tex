\chapter{RL in Classic Games}

Why are classic games such a meaningful area to apply Reinforcement Learning to? The main point is that these games usually have quite simple rules, but very deep concepts. They also provide for a meaningful IQ test, and encapsulate real world issues. This makes it a really interesting field for Reinforcement Learning.

\section{Game Theory}

In order to get a sense of optimality in multiplayer games, the field of Game Theory is often used. The aim is to find the optimal policy $\pi^i$ for the $i$-th player. If all other players fix their policies ($\pi^{-i}$), the \textbf{best response} $\pi_*^i(\pi^{-i})$ is the optimal policy against those policies.

There is a problem with this, because it basically learns the optimal policy for \textit{one} fixed policy of the opponents. Instead, the goal is to find the optimal policy against all policies opponents can have. The most well-known solution to this is referred to as the \textbf{Nash equilibrium}. This is a joint policy for all players.

\begin{equation*}
	\pi^i = \pi_*^i(\pi^{-i})
\end{equation*}

Here, every player's policy is a best response. This means no player would choose to deviate from this equilibrium.

\begin{itemize}
	\item Best Response is a solution of a single-agent RL problem.
	
	Other players become part of the environment, and the game is reduced to an MDP. The best response is then an optimal policy of this MDP.
	
	\item Nash equilibrium is a fixed-point of self-play RL
	
	Experience is generated by playing games between agents $a_1 \sim \pi^1, a_2 \sim \pi^2, ...$. Each agent learns the best response to other players. One player's policy determines another player's environment. This means all players are adapting to each other.
\end{itemize}

To keep things relatively simple, this chapter restricts itself to \textbf{two player}, \textbf{zero sum} games. Zero sum games have equal and opposite rewards $R^1 + R^2 = 0$. Methods for finding Nash equilibria in these games are discussed.

\section{Minimax Search}

In these types of games, the value function defines the expected total reward given joint policies $(\pi^1, \pi^2)$
\begin{equation*}
	v_\pi(s) = \E_\pi[G_t | S_t = s]
\end{equation*}

A \textit{minimax} value function maximizes player 1's expected return while minimizing the one from player 2. The optimal value function becomes
\begin{equation*}
	v_*(s) = \max_{\pi^1} \min_{\pi^2} v_\pi(s)
\end{equation*}

A \textit{minimax} policy $(\pi^1, \pi^2)$ is a joint policy which achieves the minimax values. For two-player zero sum games, there exists a unique minimax value function. In this case, a minimax policy is a Nash equilibrium.

There is a problem though. The search tree grown exponentially at every level. This means it is impractical to search all the way to the end of the game. A simple solution to this problem uses a value function approximation $v(s, w) \approx v_\pi(s)$ at the leaf nodes of the search tree. Then, it is possible to run minimax to some fixed depth with respect to the leaf values.

\section{Self-Play RL}

Similar to this fixed function approximation in the last section, it is possible to apply value-based RL algorithms to games of self-play. Any algorithm like $MC, TD(0), TD(\lambda), ...$ can be used. For deterministic games, it is sufficient to estimate $v_*(s)$. The idea uses the following short proof
\begin{equation*}
	\begin{aligned}
		q_*(s, a) & = \sum_{s'} P^a_{ss'} v_*(s)\\
		q_*(s, a) & = v_*(s')
	\end{aligned}
\end{equation*}

Actions are selected by min- or maximizing the value of the next state
\begin{equation*}
	\begin{aligned}
		A_t & = \arg\max_a v_*(S_t') \text{ for the maximizing player}\\
		A_t & = \arg\min_a v_*(S_t') \text{ for the minimizing player}
	\end{aligned}
\end{equation*}

This improves the joint policy for both players.

\section{Combining RL and Minimax Search}

The ideas of both these methods can be combined. This section focuses on some ideas to combine both.

\subsection{Simple TD}

As previously seen, the idea of TD is to update the value function towards the successor value. The idea of \textit{Simple TD} is to first learn the value function in TD learning. After learning, this value function can be used in minimax search (without any learning). 
\begin{equation*}
	v_+(S_t, w) = minimax_{s \in leaves(S_t)} v(s, w)
\end{equation*}

An important question arises when using this approach: can the value function be learning directly from minimax search values? This could increase performance more quickly, since it can be hard to find a terminal state without search.

\subsection{TD Root}

The idea of \textit{TD Root} is to update the value towards the successor search value. Imagine the tree is at root node $S_t$. After taking an action, $S_{t+1}$ is explored next. TD would update the value of $S_t$ towards the value of $S_{t+1}$. The value of $S_{t+1}$ equals the value of the minimax search backup, so $v(S_{t+1}, w) = v(l(S_{t+1}), w)$, where $l(s)$ is the leaf node achieving minimax value from $s$.

This was actually the first ever TD learning algorithm applied to a checkers program that learned by self-play. It was able to defeat an amateur human player. This algorithm was then used and extended to perform better.

\subsection{TD Leaf}

One idea is to think about the whole search. Instead of updating the value of $S_t$, the value of the leaf $l(S_t)$ should be updated. This is the case, because that is the node that contributed to the minimax value $v(S_t)$. This results in the back-up $v(l(S_t), w) \leftarrow v(l(S_{t+1}), w)$, instead of $v(S_t, w) \leftarrow v(l(S_{t+1}), w)$ as seen in TD Root.

\textit{Knightcap} used TD leaf in chess. It started by only knowing standard piece values, and learned weights using TD leaf. The search used $\alpha\beta$-pruning to speed up the minimax search. It ended up achieving master-level play after a small number of games. However, it was not effective in self-play or without starting from good weights. In checkers however, a similar algorithm called Chinook was able to play at superhuman level.

\subsection{TreeStrap}

There is still something to optimize. The previously algorithms use the values obtained from the minimax search only once. \textit{TreeStrap} updates search values towards deeper search values. Minimax search values are computed at \textit{all} nodes in the search. The backup $\forall s \in nodes(S_t): v(s, w) \leftarrow v(l(s), w)$ is performed.

\textit{Meep}, a chess algorithm using TreeStrap, was able to beat 13/15 international masters starting from random initial weights. It is both effective in self-play and random initial weights.

\subsection{Simulation-Based Search}

As you have seen now, self-play reinforcement learning can replace search. Next to the idea of minimax, it is also possible to simulate games of self-play from root state $S_t$, and then apply RL to this simulated experience.

Previously, a concept using Monte-Carlo Control called \textit{MCTS} was discussed. The most effective variant of this search uses the UCT-algorithm to guide exploration/exploitation in every node. It actually turns out self-play UCT converges on minimax values in zero-sum, 2-player games.

\section{RL in Imperfect-Information Games}

In imperfect information games, players have different information states and therefore separate search trees. In this tree, there is one node for each information state. These nodes summarize what a player knows (e.g. the cards they have seen).

There may be many real states that share the same information state, but there may also be aggregate states (e.g. those with a similar value).

There are multiple ways to solve for these information-state game trees.
\begin{itemize}
	\item Iterative forward-search methods 
	
	An example of this is Counterfactual Regret Minimization
	\item Self-play Reinforcement learning
	
	Here, a variant of MCTS, called Smooth UCT is discussed briefly. Applying the method to the game of Poker resulted in 3 silver medals in two- and three-player Poker. It outperformed massive-scale forward-search agents.
\end{itemize}

\subsection{Smooth UCT}

Using Monte-Carlo Tree Search in fully observable games has shown promising. However, improvements must be made to let it perform well in imperfect information games. In Poker for example, naive MCTS diverges, since its approach is just not well-fitted for these types of games.

However, a variant of UCT, inspired by a game-theoretic concept called \textbf{Fictitious Player} was implemented. The idea is that the agent learns against and to respond to opponents' \textit{average} behavior. This gives a much robust approach.

The average strategy can be extracted from nodes' action counts, $\pi_{avg}(a | s) = \frac{N(s, a)}{N(s)}$. Then, at each node, pick actions according to
\begin{equation*}
	A \sim \begin{cases}
		UCT(S), & \text{ with probability } \epsilon\\
		\pi_{avg}(.|S), & \text{ with probability } 1 - \epsilon
	\end{cases}
\end{equation*}

Empirically, in variants of Poker, smooth UCT converged to the Nash equilibrium while naive MCTS diverged.

